import streamlit as st
import pandas as pd
import numpy as np
import os
import re
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras import layers
from transformers import AutoTokenizer
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="Kurlypool ì±—ë´‡", layout="centered")
st.title("ğŸ³ Kurlypool ì±—ë´‡")
st.markdown("ë¦¬ë·° ê¸°ë°˜ ê°„í¸ì‹ ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ì— ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

# --- ì„¤ì •ê°’ ---
MAX_LEN = 80
CATEGORICAL_DIM = 64
TOKENIZER_NAME = "beomi/kcbert-base"
SBERT_MODEL_NAME = "jhgan/ko-sroberta-multitask"
BASE_PATH = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_PATH, "bert_model", "tf_model.h5")
ANSWER_CSV_PATH = os.path.join(BASE_PATH, "..", "ì±—ë´‡íŠ¹ì§•ì¶”ì¶œìµœì¢….csv")

# --- í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜ ---
def clean_text(text):
    text = re.sub(r'([a-zA-Z0-9])[^a-zA-Z0-9ê°€-í£\s]+([a-zA-Z0-9])', r'\1 \2', str(text))
    text = re.sub(r'[^a-zA-Z0-9ê°€-í£\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# --- BERT ë˜í¼ ë ˆì´ì–´ ì •ì˜ ---
class TFBertModelWrapper(layers.Layer):
    def __init__(self, model_name=None, **kwargs):
        super().__init__(**kwargs)
        from transformers import TFAutoModel
        self.bert = TFAutoModel.from_pretrained(model_name or "beomi/kcbert-base")

    def call(self, inputs):
        input_ids, attention_mask = inputs
        return self.bert({'input_ids': input_ids, 'attention_mask': attention_mask}).last_hidden_state

# --- ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ---
@st.cache_resource
def load_model_and_tokenizer():
    model = load_model(MODEL_PATH, custom_objects={"TFBertModelWrapper": TFBertModelWrapper})
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)
    return model, tokenizer

# --- SBERT ëª¨ë¸ ë¡œë”© ---
@st.cache_resource
def load_sbert():
    return SentenceTransformer(SBERT_MODEL_NAME)

# --- ë‹µë³€ ë°ì´í„° ë¡œë”© ---
@st.cache_data
def load_answer_df():
    encodings = ["utf-8", "cp949", "euc-kr", "utf-8-sig"]
    for enc in encodings:
        try:
            df = pd.read_csv(ANSWER_CSV_PATH, encoding=enc)
            if "ë‹µë³€" in df.columns:
                df.columns = df.columns.str.strip().str.lower()
                return df
        except:
            continue
    return pd.DataFrame()

# --- ì„ë² ë”© ê³„ì‚° ---
@st.cache_data
def compute_embeddings(df, sbert_model):
    texts = df["ë‹µë³€"].astype(str).tolist()
    embeddings = sbert_model.encode(texts, convert_to_tensor=False, show_progress_bar=False)
    return texts, embeddings

# --- ì˜ë„ ì˜ˆì¸¡ í•¨ìˆ˜ ---
def predict_intent(text, model, tokenizer):
    clean = clean_text(text)
    tokens = tokenizer([clean], padding="max_length", truncation=True, max_length=MAX_LEN, return_tensors="tf")
    dummy_cat = np.zeros((1, CATEGORICAL_DIM))
    pred = model.predict([tokens["input_ids"], tokens["attention_mask"], dummy_cat], verbose=0)
    return "RECOMMEND" if np.argmax(pred) == 0 else "TREND"

# --- ìœ ì‚¬ ë‹µë³€ ì¶”ì¶œ ---
def get_best_answer(query, texts, embeddings, sbert_model):
    query_vec = sbert_model.encode([query], convert_to_tensor=False)
    sims = cosine_similarity(query_vec, embeddings)[0]
    best_idx = np.argmax(sims)
    return texts[best_idx]

# --- Streamlit UI ---
user_input = st.text_input("â“ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:")

if st.button("ë‹µë³€ ë°›ê¸°") and user_input.strip():
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        model, tokenizer = load_model_and_tokenizer()
        sbert_model = load_sbert()
        answer_df = load_answer_df()
        texts, emb = compute_embeddings(answer_df, sbert_model)

        intent = predict_intent(user_input, model, tokenizer)
        answer = get_best_answer(user_input, texts, emb, sbert_model)

        st.markdown(f"**ì˜ˆì¸¡ëœ ì˜ë„:** `{intent}`")
        st.markdown(f"**ì±—ë´‡ ì‘ë‹µ:** {answer}")
