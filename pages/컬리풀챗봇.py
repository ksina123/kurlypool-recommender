import streamlit as st
import pandas as pd
import numpy as np
import os
import re
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModel
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from tensorflow.keras import layers
from tensorflow.keras.models import Model

# --- BERT ë˜í•‘ ë ˆì´ì–´ ì •ì˜ ---
class TFBertModelWrapper(layers.Layer):
    def __init__(self, model_name="beomi/kcbert-base", **kwargs):
        super(TFBertModelWrapper, self).__init__(**kwargs)
        self.bert = TFAutoModel.from_pretrained(model_name)

    def call(self, inputs):
        input_ids, attention_mask = inputs
        outputs = self.bert({'input_ids': input_ids, 'attention_mask': attention_mask})
        return outputs.last_hidden_state

# --- ëª¨ë¸ ìƒì„± í•¨ìˆ˜ (2-input: BERT + CNN) ---
def create_model():
    max_len = 80
    input_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_ids")
    attention_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="attention_mask")

    bert_wrapper = TFBertModelWrapper("beomi/kcbert-base")
    bert_output = bert_wrapper([input_ids, attention_mask])

    cnn_out = tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu')(bert_output)
    cnn_out = tf.keras.layers.GlobalMaxPooling1D()(cnn_out)

    fc = tf.keras.layers.Dense(64, activation='relu')(cnn_out)
    output = tf.keras.layers.Dense(2, activation='softmax')(fc)

    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)
    return model

# --- ê²½ë¡œ ì„¤ì • ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
WEIGHT_PATH = os.path.join(CURRENT_DIR, "..", "0715_intent_model_final.h5")
TOKENIZER_NAME = "beomi/kcbert-base"
SBERT_MODEL = "jhgan/ko-sroberta-multitask"
CSV_FILES = {"TREND": "ì±—ë´‡íŠ¹ì§•ì¶”ì¶œìµœì¢….csv"}

# --- ëª¨ë¸ ë° ë¦¬ì†ŒìŠ¤ ë¡œë”© ---
@st.cache_resource
def load_intent_model():
    model = create_model()
    model.load_weights(WEIGHT_PATH)
    return model

@st.cache_resource
def load_tokenizer():
    return AutoTokenizer.from_pretrained(TOKENIZER_NAME)

@st.cache_resource
def load_sbert():
    return SentenceTransformer(SBERT_MODEL)

@st.cache_data
def load_answer_dfs():
    dfs = {}
    for key, path in CSV_FILES.items():
        if os.path.exists(path):
            encodings = ['utf-8', 'cp949', 'euc-kr', 'utf-8-sig']
            for enc in encodings:
                try:
                    df = pd.read_csv(path, encoding=enc)
                    df.columns = df.columns.str.lower().str.strip()
                    dfs[key] = df
                    break
                except:
                    continue
    return dfs

# --- í…ìŠ¤íŠ¸ ì •ì œ ---
def clean_text(text):
    text = re.sub(r'([a-zA-Z0-9])[^a-zA-Z0-9ê°€-í£\s]+([a-zA-Z0-9])', r'\1 \2', str(text))
    text = re.sub(r'[^a-zA-Z0-9ê°€-í£\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# --- ì˜ë„ ì˜ˆì¸¡ ---
def predict_intent(user_input, model, tokenizer):
    text = clean_text(user_input)
    X_input = tokenizer([text], padding='max_length', truncation=True, max_length=80, return_tensors='tf')
    pred = model.predict([X_input['input_ids'], X_input['attention_mask']], verbose=0)
    idx2label = {0: "RECOMMEND", 1: "TREND"}
    return idx2label.get(np.argmax(pred), "TREND")

# --- intentì— ë§ëŠ” DataFrame ì„ íƒ ---
def select_df_by_intent(intent, dfs):
    return dfs.get("TREND", None)

# --- ì‚¬ì „ ì„ë² ë”© ê³„ì‚° ---
@st.cache_data
def precompute_all_embeddings(dfs, sbert_model):
    emb_dict = {}
    for key, df in dfs.items():
        if 'ë‹µë³€' in df.columns and len(df):
            texts = df['ë‹µë³€'].astype(str).tolist()
            emb = sbert_model.encode(texts, convert_to_tensor=False, show_progress_bar=False)
            emb_dict[key] = {"emb": emb, "texts": texts}
    return emb_dict

# --- ìœ ì‚¬ë„ ê¸°ë°˜ ìµœì  ë‹µë³€ ì¶”ì¶œ ---
def get_best_answer(user_input, answer_df, emb_dict, sbert_model):
    if answer_df is None or len(answer_df) == 0:
        return "ë‹µë³€ í›„ë³´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    key = next((k for k, v in emb_dict.items() if len(v['texts']) == len(answer_df)), None)
    if key is None:
        return "ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    texts = emb_dict[key]['texts']
    q_embeddings = emb_dict[key]['emb']
    user_emb = sbert_model.encode([user_input], convert_to_tensor=False)
    sims = cosine_similarity(user_emb, q_embeddings)[0]
    return texts[np.argmax(sims)]

# --- ì „ì²´ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ---
def process_user_input(user_input, intent_model, tokenizer, dfs, emb_dict, sbert_model):
    intent = predict_intent(user_input, intent_model, tokenizer)
    df = select_df_by_intent(intent, dfs)
    answer = get_best_answer(user_input, df, emb_dict, sbert_model)
    return answer, intent

# --- Streamlit UI êµ¬ì„± ---
st.set_page_config(page_title="Kurlypool ì±—ë´‡", layout="centered")
st.title("ğŸ³ Kurlypool ì±—ë´‡")
st.markdown("ë¦¬ë·° ê¸°ë°˜ ê°„í¸ì‹ ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ì— ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

try:
    intent_model = load_intent_model()
    tokenizer = load_tokenizer()
    sbert_model = load_sbert()
    dfs = load_answer_dfs()
    emb_dict = precompute_all_embeddings(dfs, sbert_model)

    user_input = st.text_input("â“ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:")

    if st.button("ë‹µë³€ ë°›ê¸°") and user_input.strip():
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            answer, intent = process_user_input(user_input, intent_model, tokenizer, dfs, emb_dict, sbert_model)
            st.markdown(f"**ì˜ˆì¸¡ëœ ì˜ë„:** `{intent}`")
            st.markdown(f"**ì±—ë´‡ ì‘ë‹µ:** {answer}")

except Exception as e:
    st.error("â—ï¸ëª¨ë¸ ë˜ëŠ” ë¦¬ì†ŒìŠ¤ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
    st.exception(e)
