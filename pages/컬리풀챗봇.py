import streamlit as st
import pandas as pd
import numpy as np
import os
import re
import tensorflow as tf
from tensorflow.keras import layers, Model, Input
from transformers import TFAutoModel, AutoTokenizer
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# --- ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(page_title="Kurlypool ì±—ë´‡", layout="centered")
st.title("ğŸ³ Kurlypool ì±—ë´‡")
st.markdown("ë¦¬ë·° ê¸°ë°˜ ê°„í¸ì‹ ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ì— ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
MAX_LEN = 80
CATEGORICAL_DIM = 64
TOKENIZER_NAME = "beomi/kcbert-base"
SBERT_MODEL_NAME = "jhgan/ko-sroberta-multitask"
BASE_PATH = os.path.dirname(os.path.abspath(__file__))
WEIGHT_PATH = os.path.join(BASE_PATH, "bert_model", "tf_model.h5")
ANSWER_CSV_PATH = os.path.join(BASE_PATH, "..", "ì±—ë´‡íŠ¹ì§•ì¶”ì¶œìµœì¢….csv")

# --- í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜ ---
def clean_text(text):
    text = re.sub(r'([a-zA-Z0-9])[^a-zA-Z0-9ê°€-í£\s]+([a-zA-Z0-9])', r'\1 \2', str(text))
    text = re.sub(r'[^a-zA-Z0-9ê°€-í£\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# --- ì»¤ìŠ¤í…€ BERT ë˜í¼ ë ˆì´ì–´ ---
class TFBertModelWrapper(layers.Layer):
    def __init__(self, model_name, **kwargs):
        super().__init__(**kwargs)
        self.bert = TFAutoModel.from_pretrained(model_name)

    def call(self, inputs):
        input_ids, attention_mask = inputs
        outputs = self.bert({'input_ids': input_ids, 'attention_mask': attention_mask})
        return outputs.last_hidden_state

# --- ëª¨ë¸ ìƒì„± í•¨ìˆ˜ ---
def create_model():
    input_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name="input_ids")
    attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name="attention_mask")
    categorical_features = Input(shape=(CATEGORICAL_DIM,), name="categorical_features")

    bert_wrapper = TFBertModelWrapper(TOKENIZER_NAME)
    bert_output = bert_wrapper([input_ids, attention_mask])

    cnn_out = layers.Conv1D(128, kernel_size=3, activation='relu')(bert_output)
    cnn_out = layers.GlobalMaxPooling1D()(cnn_out)

    merged = layers.Concatenate()([cnn_out, categorical_features])
    fc = layers.Dense(64, activation='relu')(merged)
    output = layers.Dense(2, activation='softmax')(fc)

    return Model(inputs=[input_ids, attention_mask, categorical_features], outputs=output)

# --- ëª¨ë¸ + í† í¬ë‚˜ì´ì € ë¡œë”© ---
@st.cache_resource
def load_model_and_tokenizer():
    model = create_model()
    model.load_weights(WEIGHT_PATH)
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)
    return model, tokenizer

# --- SBERT ë¡œë”© ---
@st.cache_resource
def load_sbert():
    return SentenceTransformer(SBERT_MODEL_NAME)

# --- ë‹µë³€ CSV ë¡œë”© ---
@st.cache_data
def load_answer_df():
    encodings = ["utf-8", "cp949", "euc-kr", "utf-8-sig"]
    for enc in encodings:
        try:
            df = pd.read_csv(ANSWER_CSV_PATH, encoding=enc)
            if "ë‹µë³€" in df.columns:
                df.columns = df.columns.str.strip().str.lower()
                return df
        except:
            continue
    return pd.DataFrame()

# --- ì„ë² ë”© ê³„ì‚° ---
@st.cache_data
def compute_embeddings(df, sbert_model):
    texts = df["ë‹µë³€"].astype(str).tolist()
    embeddings = sbert_model.encode(texts, convert_to_tensor=False, show_progress_bar=False)
    return texts, embeddings

# --- ì˜ë„ ì˜ˆì¸¡ ---
def predict_intent(text, model, tokenizer):
    clean = clean_text(text)
    tokens = tokenizer([clean], padding="max_length", truncation=True, max_length=MAX_LEN, return_tensors="tf")
    dummy_cat = np.zeros((1, CATEGORICAL_DIM))
    pred = model.predict([tokens["input_ids"], tokens["attention_mask"], dummy_cat], verbose=0)
    return "RECOMMEND" if np.argmax(pred) == 0 else "TREND"

# --- ë‹µë³€ ì¶”ì¶œ ---
def get_best_answer(query, texts, embeddings, sbert_model):
    query_vec = sbert_model.encode([query], convert_to_tensor=False)
    sims = cosine_similarity(query_vec, embeddings)[0]
    best_idx = np.argmax(sims)
    return texts[best_idx]

# --- Streamlit ì¸í„°í˜ì´ìŠ¤ ---
user_input = st.text_input("â“ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:")

if st.button("ë‹µë³€ ë°›ê¸°") and user_input.strip():
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        model, tokenizer = load_model_and_tokenizer()
        sbert_model = load_sbert()
        answer_df = load_answer_df()
        texts, emb = compute_embeddings(answer_df, sbert_model)

        intent = predict_intent(user_input, model, tokenizer)
        answer = get_best_answer(user_input, texts, emb, sbert_model)

        st.markdown(f"**ì˜ˆì¸¡ëœ ì˜ë„:** `{intent}`")
        st.markdown(f"**ì±—ë´‡ ì‘ë‹µ:** {answer}")
