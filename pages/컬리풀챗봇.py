import streamlit as st
import pandas as pd
import numpy as np
import os
import re
import tensorflow as tf
import time
from transformers import AutoTokenizer, TFAutoModel
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from tensorflow.keras import layers

# --- ì»¤ìŠ¤í…€ BERT ë ˆì´ì–´ ì •ì˜ ---
class TFBertModelWrapper(layers.Layer):
    def __init__(self, model_name="beomi/kcbert-base", **kwargs):
        super().__init__(**kwargs)
        self.bert = TFAutoModel.from_pretrained(model_name)
    def call(self, inputs):
        input_ids, attention_mask = inputs
        outputs = self.bert({'input_ids': input_ids, 'attention_mask': attention_mask})
        return outputs.last_hidden_state

# --- ëª¨ë¸ êµ¬ì¡° ì •ì˜ ---
def create_model():
    input_ids = tf.keras.Input(shape=(80,), dtype=tf.int32, name="input_ids")
    attention_mask = tf.keras.Input(shape=(80,), dtype=tf.int32, name="attention_mask")
    categorical_features = tf.keras.Input(shape=(64,), dtype=tf.float32, name="categorical_features")

    bert_output = TFBertModelWrapper("beomi/kcbert-base")([input_ids, attention_mask])
    pooled_output = tf.keras.layers.GlobalAveragePooling1D()(bert_output)

    x = tf.keras.layers.concatenate([pooled_output, categorical_features])
    x = tf.keras.layers.Dense(128, activation='relu')(x)
    x = tf.keras.layers.Dense(3, activation='softmax')(x)

    model = tf.keras.Model(inputs=[input_ids, attention_mask, categorical_features], outputs=x)
    return model

# --- ì„¤ì • ---
WEIGHT_PATH = "pages/bert_model/intent_model.weights.h5"
TOKENIZER_NAME = "beomi/kcbert-base"
SBERT_MODEL = "jhgan/ko-sroberta-multitask"
CSV_FILES = {"TREND": "ì±—ë´‡íŠ¹ì§•ì¶”ì¶œìµœì¢….csv"}

# --- ëª¨ë¸ ë° ë¦¬ì†ŒìŠ¤ ë¡œë”© í•¨ìˆ˜ ---
@st.cache_resource
def load_intent_model():
    model = create_model()
    model.load_weights(WEIGHT_PATH)
    return model

@st.cache_resource
def load_tokenizer():
    return AutoTokenizer.from_pretrained(TOKENIZER_NAME)

@st.cache_resource
def load_sbert():
    return SentenceTransformer(SBERT_MODEL)

@st.cache_data
def load_answer_dfs():
    dfs = {}
    for key, path in CSV_FILES.items():
        if os.path.exists(path):
            encodings = ['utf-8', 'cp949', 'euc-kr', 'utf-8-sig']
            df = None
            for enc in encodings:
                try:
                    df = pd.read_csv(path, encoding=enc)
                    break
                except:
                    continue
            if df is not None:
                df.columns = df.columns.str.lower().str.strip()
                dfs[key] = df
    return dfs

# --- í…ìŠ¤íŠ¸ ì •ì œ ---
def clean_text(text):
    text = re.sub(r'([a-zA-Z0-9])[^a-zA-Z0-9ê°€-í£\s]+([a-zA-Z0-9])', r'\1 \2', str(text))
    text = re.sub(r'[^a-zA-Z0-9ê°€-í£\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# --- ì˜ë„ ì˜ˆì¸¡ ---
def predict_intent(user_input, model, tokenizer):
    text = clean_text(user_input)
    X_input = tokenizer([text], padding='max_length', truncation=True, max_length=80, return_tensors='tf')
    dummy_cat = np.zeros((1, 64))
    pred = model.predict([X_input['input_ids'], X_input['attention_mask'], dummy_cat], verbose=0)
    idx2label = {0: "RECOMMEND", 1: "TREND", 2: "NEGFAQ"}
    intent_idx = np.argmax(pred, axis=1)[0]
    return idx2label.get(intent_idx, "TREND")

# --- intentë³„ ë°ì´í„° ì„ íƒ ---
def select_df_by_intent(intent, dfs):
    return dfs.get("TREND")

# --- ì„ë² ë”© ì‚¬ì „ ê³„ì‚° ---
@st.cache_data
def precompute_all_embeddings(dfs, sbert_model):
    emb_dict = {}
    for key, df in dfs.items():
        if 'ë‹µë³€' in df.columns and len(df):
            texts = df['ë‹µë³€'].astype(str).tolist()
            emb = sbert_model.encode(texts, convert_to_tensor=False, show_progress_bar=False)
            emb_dict[key] = {"emb": emb, "texts": texts}
    return emb_dict

# --- ìœ ì‚¬ë„ ê¸°ë°˜ ìµœì  ë‹µë³€ ì¶”ì¶œ ---
def get_best_answer(user_input, answer_df, emb_dict, sbert_model):
    if answer_df is None or len(answer_df) == 0:
        return "ë‹µë³€ í›„ë³´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    key = None
    for k, v in emb_dict.items():
        if answer_df is not None and 'texts' in v and len(v['texts']) == len(answer_df):
            key = k
            break
    if key is None:
        return "ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    texts = emb_dict[key]['texts']
    q_embeddings = emb_dict[key]['emb']
    user_emb = sbert_model.encode([user_input], convert_to_tensor=False)
    sims = cosine_similarity(user_emb, q_embeddings)[0]
    best_idx = sims.argmax()
    return texts[best_idx]

# --- ì „ì²´ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ---
def process_user_input(user_input, intent_model, tokenizer, dfs, emb_dict, sbert_model):
    intent = predict_intent(user_input, intent_model, tokenizer)
    df = select_df_by_intent(intent, dfs)
    answer = get_best_answer(user_input, df, emb_dict, sbert_model)
    return answer, intent

# --- Streamlit UI ---
st.set_page_config(page_title="Kurlypool ì±—ë´‡", layout="centered")
st.title("ğŸ³ Kurlypool ì±—ë´‡")
st.markdown("ë¦¬ë·° ê¸°ë°˜ ê°„í¸ì‹ ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ì— ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

# ë¦¬ì†ŒìŠ¤ ë¡œë”©
intent_model = load_intent_model()
tokenizer = load_tokenizer()
sbert_model = load_sbert()
dfs = load_answer_dfs()
emb_dict = precompute_all_embeddings(dfs, sbert_model)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
user_input = st.text_input("â“ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:")

if st.button("ë‹µë³€ ë°›ê¸°") and user_input.strip():
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        answer, intent = process_user_input(user_input, intent_model, tokenizer, dfs, emb_dict, sbert_model)
        st.markdown(f"**ì˜ˆì¸¡ëœ ì˜ë„:** `{intent}`")
        st.markdown(f"**ì±—ë´‡ ì‘ë‹µ:** {answer}")
